[core]
executor = CeleryExecutor
dags_folder = /opt/airflow/dags
plugins_folder = /opt/airflow/plugins
base_log_folder = /opt/airflow/logs
fernet_key = ${FERNET_KEY}
secret_key = ${FERNET_KEY}
load_examples = False
dags_are_paused_at_creation = True
min_serialized_dag_update_interval = 30
check_slas = True
parallelism = 16
dag_concurrency = 8
max_active_runs_per_dag = 4
max_active_tasks_per_dag = 16

[database]
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow
sql_engine_encoding = utf-8
pool_size = 10
max_overflow = 20
pool_recycle = 1800

[webserver]
base_url = http://localhost:8080
web_server_port = 8080
web_server_host = 0.0.0.0
worker_refresh_interval = 30
workers = 4
worker_class = sync
rbac = False

[scheduler]
max_threads = 4
scheduler_heartbeat_sec = 5
catchup_by_default = False
schedule_after_task_execution = True
parsing_processes = 4
scheduler_health_check_threshold = 30

[celery]
broker_url = redis://redis:6379/0
result_backend = db+postgresql://airflow:airflow@postgres/airflow
worker_concurrency = 4
worker_prefetch_multiplier = 1
worker_autoscale = 4,8
task_acks_late = True
worker_cancel_long_running_tasks = True
task_time_limit = 1800
task_soft_time_limit = 1500
broker_transport_options = {"visibility_timeout": 21600, "socket_keepalive": true, "retry_on_timeout": true}
sync_parallelism = 1
disable_rate_limits = True

[logging]
remote_logging = False
logging_level = DEBUG
fab_logging_level = WARN
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[api]
auth_backends = airflow.api.auth.backend.basic_auth
enable_experimental_api = False

[elasticsearch]
host =

[kerberos]
enabled = False

[metrics]
statsd_on = False

[secrets]
backend =

[smtp]
smtp_host =
smtp_starttls = False
smtp_ssl = False

[elasticsearch_configs]
use_ssl = False
verify_certs = False